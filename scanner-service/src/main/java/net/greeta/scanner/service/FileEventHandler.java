package net.greeta.scanner.service;

import com.fasterxml.jackson.databind.ObjectMapper;
import net.greeta.scanner.data.EventDto;
import net.greeta.scanner.repository.FileEventResultRepository;
import net.greeta.scanner.util.FileUtil;
import java.io.File;
import java.nio.file.Files;

import jakarta.annotation.PostConstruct;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;
import org.springframework.stereotype.Component;
import org.springframework.util.FileSystemUtils;

@Component
public class FileEventHandler {
  @Autowired ObjectMapper objectMapper;
  @Autowired
  FileService fileService;
  @Autowired
  FileEventResultRepository eventResultRepository;
  private static final Logger logger = LoggerFactory.getLogger(FileEventHandler.class);
  @Autowired ThreadPoolTaskExecutor executor;

  @PostConstruct
  private void init() {
    executor.setCorePoolSize(1);
  }

  @Autowired KafkaManager kafkaManager;

  @KafkaListener(
      id = "file_event_consumer",
      topics = "spring-file.public.file_event",
      groupId = "spring-file.public.file_event:consumer",
      concurrency = "1")
  void consume(String message, Acknowledgment acknowledgment) {
    kafkaManager.pauseConsume("file_event_consumer");
    executor
        .submitListenable(
            () -> {
              try {
                process(message, acknowledgment);
              } catch (Exception e) {
                throw new RuntimeException(e);
              }
            })
        .addCallback(
            (res) -> {
              kafkaManager.resumeConsumer("file_event_consumer");
            },
            ex -> {
              logger.info(ex.getMessage());
              try {
                Thread.sleep(15000);
              } catch (InterruptedException e) {
                throw new RuntimeException(e);
              }
              kafkaManager.resumeConsumer("file_event_consumer");
              kafkaManager.restartConsumer("file_event_consumer");
            });
  }

  private void process(String message, Acknowledgment acknowledgment) {
    // read event
    EventDto eventDto;
    try {
      eventDto = objectMapper.readValue(message, EventDto.class);
    } catch (Exception e) {
      e.printStackTrace();
      logger.info(e.getMessage());
      throw new RuntimeException("cannot deserialize file event: " + message);
    }
    logger.info(eventDto.getEvent().toString());

    // check processed before
    if (eventResultRepository.existsById(eventDto.getEvent().getId())) {
      acknowledgment.acknowledge();
      logger.info("duplicated event !");
      return;
    }
    // Thread.sleep(1000*60*10);

    // download
    File file;
    try {
      file = fileService.download(eventDto.getEvent().getSource(), eventDto.getEvent().getId());
      file.deleteOnExit();
    } catch (Exception e) {
      e.printStackTrace();
      fileService.handleFailure(eventDto, "cannot retrieve file !");
      acknowledgment.acknowledge();
      return;
    }

    // virus check
    try {
      fileService.scan(file);
    } catch (Exception e) {
      e.printStackTrace();
      fileService.handleFailure(eventDto, "virus check failed!");
      acknowledgment.acknowledge();
      file.delete();
      return;
    }

    // upload scanned file
    String streamPath =
        "user/"
            + eventDto.getEvent().getUserId()
            + "/file/"
            + eventDto.getEvent().getFileId()
            + "/";
    try {
      fileService.upload(file, streamPath);
    } catch (Exception e) {
      e.printStackTrace();
      fileService.handleFailure(eventDto, "cannot upload scanned file !");
      acknowledgment.acknowledge();
      file.delete();
      return;
    }

    // success
    fileService.handleSuccess(eventDto, streamPath);
    acknowledgment.acknowledge();
    file.delete();
  }
}
